{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9de89ad-35e9-47fe-8b9d-f00fabcb9e31",
   "metadata": {},
   "source": [
    "\n",
    "====== Original content created by Cam Davidson-Pilon\n",
    "Ported to Python 3 and PyMC3 by Max Margenot (@clean_utensils) and Thomas Wiecki (@twiecki) at Quantopian (@quantopian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e7b65a-0362-43c5-ab0a-cb5e280e65b9",
   "metadata": {},
   "source": [
    "### Coin-flipping example\n",
    "\n",
    "Suppose, naively, that you are unsure about the probability of heads in a coin flip (spoiler alert: it's 50 \\%). You believe there is some true underlying ratio, call it $p$, but have no prior opinion on what $p$ might be.\n",
    "\n",
    "We begin to flip a coin, and record the observations: either $H$ or $T$. This is our observed data. An interesting question to ask is how our inference changes as we observe more and more data? More specifically, what do our posterior probabilities look like when we have little data, versus when we have lots of data.\n",
    "\n",
    "Below we plot a sequence of updating posterior probabilities as we observe increasing amounts of data (coin flips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d055a-7ba4-416d-bf06-777ccc5334fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "figsize(11, 9)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "dist = stats.beta\n",
    "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 500]\n",
    "data = stats.bernoulli.rvs(0.5, size=n_trials[-1])\n",
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "# # For the already prepared, I'm using Binomial's conj. prior.\n",
    "for k, N in enumerate(n_trials):\n",
    "    sx = plt.subplot(int(len(n_trials) / 2), 2, k + 1)\n",
    "    plt.xlabel(\"$p$, probability of heads\") \n",
    "    #if k in [0, len(n_trials) - 1] else None\n",
    "    plt.setp(sx.get_yticklabels(), visible=False)\n",
    "    heads = data[:N].sum()\n",
    "    y = dist.pdf(x, 1 + heads+1, 1 + N - heads+1) #by varying parameters of the beta distribution we can see how prior influences posterior\n",
    "    plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
    "    plt.fill_between(x, 0, y, color=\"#348ABD\", alpha=0.4)\n",
    "    plt.vlines(0.5, 0, 4, color=\"k\", linestyles=\"--\", lw=1)\n",
    "\n",
    "    leg = plt.legend()\n",
    "    leg.get_frame().set_alpha(0.4)\n",
    "    plt.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f64d7a-2ee1-4e39-a8e5-ab6f2d37c9f5",
   "metadata": {},
   "source": [
    "### Continuous distributions\n",
    "\n",
    "An example of continuous random variable is a random variable with *exponential density*. The density function for an exponential random variable looks like this:\n",
    "\n",
    "$p_X(x|\\lambda)=\\lambda e^{-\\lambda x}$\n",
    "\n",
    "\n",
    "An exponential random variable can take on *any* non-negative values. This property makes it a poor choice for count data, which must be an integer, but a great choice for time data, temperature data (measured in Kelvins, of course), or any other precise *and positive* variable. The graph below shows two probability density functions with different $\\lambda$ values. \n",
    "\n",
    "When a random variable $X$ has an exponential distribution with parameter $\\lambda$, we say *$X$ is exponential* and write\n",
    "\n",
    "$$X\\sim \\text{Exp}(\\lambda)$$\n",
    "\n",
    "Given a specific $\\lambda$, the expected value of an exponential random variable is equal to the inverse of $\\lambda$, that is:\n",
    "\n",
    "$$E[\\; X \\;|\\; \\lambda \\;] = \\frac{1}{\\lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52f2f83-93db-456a-93ca-eb77b6320475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "a = np.arange(16)\n",
    "colours = [\"#348ABD\", \"#A60628\"]\n",
    "\n",
    "a = np.linspace(0, 4, 100)\n",
    "expo = stats.expon\n",
    "lambda_ = [0.5, 0.1] #vary lambda to see other types of distributions\n",
    "\n",
    "for l, c in zip(lambda_, colours):\n",
    "    plt.plot(a, expo.pdf(a, scale=1. / l), lw=3,\n",
    "             color=c, label=\"$\\lambda = %.1f$\" % l)\n",
    "    plt.fill_between(a, expo.pdf(a, scale=1. / l), color=c, alpha=.33)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"PDF at $z$\")\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.title(\"Probability density function of an Exponential random variable;\\\n",
    " differing $\\lambda$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a01ff6-0431-477d-aeef-d50bcca6ef37",
   "metadata": {},
   "source": [
    "### But what is $\\lambda \\;$?\n",
    "\n",
    "\n",
    "Bayesian inference is concerned with *beliefs* about what $\\lambda$ might be. Rather than try to guess $\\lambda$ exactly, we can only talk about what $\\lambda$ is likely to be by assigning a probability distribution to $\\lambda$.\n",
    "\n",
    "This might seem odd at first. After all, $\\lambda$ is fixed; it is not (necessarily) random! How can we assign probabilities to values of a non-random variable? Ah, we have fallen for our old, frequentist way of thinking. Under Bayesian philosophy, we *can* assign probabilities if we interpret them as beliefs. And it is entirely acceptable to have *beliefs* about the parameter $\\lambda$. \n",
    "\n",
    "Besides that, once we have a distribution we can always derive a point estimate out of it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e30fe9a-fc67-4bf8-a93c-fce03b2a33a7",
   "metadata": {},
   "source": [
    "##### Example: Inferring behaviour from text-message data\n",
    "\n",
    "Let's try to model a more interesting example, one that concerns the rate at which a user sends and receives text messages:\n",
    "\n",
    ">  You are given a series of daily text-message counts from a user of your system. The data, plotted over time, appears in the chart below. You are curious to know if the user's text-messaging habits have changed over time, either gradually or suddenly. How can you model this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8135af-2a6a-4781-91d6-8049d0b1eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 3.5)\n",
    "count_data = np.loadtxt(\"data/txtdata.csv\")\n",
    "n_count_data = len(count_data)\n",
    "plt.bar(np.arange(n_count_data), count_data, color=\"#348ABD\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"count of text-msgs received\")\n",
    "plt.title(\"Did the user's texting habits change over time?\")\n",
    "plt.xlim(0, n_count_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a5188-2135-44ed-9dce-4ab9358d970d",
   "metadata": {},
   "source": [
    "Before we start modeling, see what you can figure out just by looking at the chart above. Would you say there was a change in behaviour during this time period? \n",
    "\n",
    "How can we start to model this?\n",
    "\n",
    "The count data are discrete and hence for modelling them a discrete distribution would be more appropriate.\n",
    "\n",
    "A fairly generic distribution for modelling count data is the Poisson distribution. It is defined as\n",
    "\n",
    "$$P(X = k) =\\frac{ \\lambda^k e^{-\\lambda} }{k!}, \\; \\; k=0,1,2, \\dots, \\; \\; \\lambda \\in \\mathbb{R}_{>0} $$\n",
    "\n",
    "$\\lambda$ is called a parameter of the distribution, and it controls the distribution's shape. For the Poisson distribution, $\\lambda$ can be any positive number. By increasing $\\lambda$, we add more probability to larger values, and conversely by decreasing $\\lambda$ we add more probability to smaller values. One can describe $\\lambda$ as the *intensity* of the Poisson distribution. \n",
    "\n",
    "Unlike $\\lambda$, which can be any positive number, the value $k$ in the above formula must be a non-negative integer, i.e., $k$ must take on values 0,1,2, and so on. \n",
    "\n",
    "If a random variable $X$ has a Poisson mass distribution, we denote this by writing\n",
    "\n",
    "$$X \\sim \\text{Poi}(\\lambda) $$\n",
    "\n",
    "One useful property of the Poisson distribution is that its expected value is equal to its parameter, i.e.:\n",
    "\n",
    "$$E\\large[ \\;X\\; | \\; \\lambda \\;\\large] = \\lambda $$\n",
    "\n",
    "We will use this property often, so it's useful to remember. Below, we plot the probability mass distribution for different $\\lambda$ values. The first thing to notice is that by increasing $\\lambda$, we add more probability of larger values occurring. Second, notice that although the graph ends at 15, the distributions do not. They assign positive probability to every non-negative integer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408061b-a7a0-466c-8633-6cf9b9d13a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 4)\n",
    "\n",
    "import scipy.stats as stats\n",
    "a = np.arange(16)\n",
    "poi = stats.poisson\n",
    "lambda_ = [1.5, 4.25]\n",
    "colours = [\"#348ABD\", \"#A60628\"]\n",
    "\n",
    "plt.bar(a, poi.pmf(a, lambda_[0]), color=colours[0],\n",
    "         label=\"$\\lambda = %.1f$\" % lambda_[0], alpha=0.60,\n",
    "         edgecolor=colours[0], \n",
    "        lw=3\n",
    "       )\n",
    "\n",
    "plt.bar(a, poi.pmf(a, lambda_[1]), color=colours[1],\n",
    "         label=\"$\\lambda = %.1f$\" % lambda_[1], alpha=0.60,\n",
    "        edgecolor=colours[1], lw=3)\n",
    "\n",
    "plt.xticks(a + 0.4, a)\n",
    "plt.legend()\n",
    "plt.ylabel(\"probability of $k$\")\n",
    "plt.xlabel(\"$k$\")\n",
    "plt.title(\"Probability mass function of a Poisson random variable; differing \\\n",
    "$\\lambda$ values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca3e431-536f-4f9e-b081-55e74050148c",
   "metadata": {},
   "source": [
    "Coming back to the text message task, one can see that a Poisson random variable would be a very appropriate model for this type of *count* data. Denoting day $i$'s text-message count by $C_i$, \n",
    "\n",
    "$$ C_i \\sim \\text{Poisson}(\\lambda)  $$\n",
    "\n",
    "We are not sure what the value of the $\\lambda$ parameter really is, however. Looking at the chart above, it appears that the rate might become higher late in the observation period, which is equivalent to saying that $\\lambda$ increases at some point during the observations. (Recall that a higher value of $\\lambda$ assigns more probability to larger outcomes. That is, there is a higher probability of many text messages having been sent on a given day.)\n",
    "\n",
    "How can we represent this observation mathematically? Let's assume that on some day during the observation period (call it $\\tau$), the parameter $\\lambda$ suddenly jumps to a higher value. So we really have two $\\lambda$ parameters: one for the period before $\\tau$, and one for the rest of the observation period. In the literature, a sudden transition like this would be called a *change-point*:\n",
    "\n",
    "$$\n",
    "\\lambda = \n",
    "\\begin{cases}\n",
    "\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n",
    "\\lambda_2 & \\text{if } t \\ge \\tau\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "If, in reality, no sudden change occurred and indeed $\\lambda_1 = \\lambda_2$, then the $\\lambda$s posterior distributions should look about equal.\n",
    "\n",
    "We are interested in inferring the unknown $\\lambda$s. To use Bayesian inference, we need to assign prior probabilities to the different possible values of $\\lambda$. What would be good prior probability distributions for $\\lambda_1$ and $\\lambda_2$? Recall that $\\lambda$ can be any positive number. As we saw earlier, the *exponential* distribution provides a continuous density function for positive numbers, so it might be a good choice for modeling $\\lambda_i$. But recall that the exponential distribution takes a parameter of its own, so we'll need to include that parameter in our model. Let's call that parameter $\\alpha$.\n",
    "\n",
    "\\begin{align}\n",
    "&\\lambda_1 \\sim \\text{Exp}( \\alpha ) \\\\\\\n",
    "&\\lambda_2 \\sim \\text{Exp}( \\alpha )\n",
    "\\end{align}\n",
    "\n",
    "$\\alpha$ is called a *hyper-parameter* or *parent variable*. In literal terms, it is a parameter that influences other parameters. Our initial guess at $\\alpha$ does not influence the model too strongly, so we have some flexibility in our choice.  A good rule of thumb is to set the exponential parameter equal to the inverse of the average of the count data. Since we're modeling $\\lambda$ using an exponential distribution, we can use the expected value identity shown earlier to get:\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=0}^N \\;C_i \\approx E[\\; \\lambda \\; |\\; \\alpha ] = \\frac{1}{\\alpha}$$ \n",
    "\n",
    "\n",
    "What about $\\tau$? Because of the noisiness of the data, it's difficult to pick out a priori when $\\tau$ might have occurred. Instead, we can assign a *uniform prior belief* to every possible day. This is equivalent to saying\n",
    "\n",
    "\\begin{align}\n",
    "& \\tau \\sim \\text{DiscreteUniform(1,70) }\\\\\\\\\n",
    "& \\Rightarrow P( \\tau = k ) = \\frac{1}{70}\n",
    "\\end{align}\n",
    "\n",
    "Given all the information above combined with the Bayes rule we can rigorously derive overall prior distribution of the parameters as well as the likelihood, however given that this course concentrates more on deploying Bayesian models rather than on mathematical aspects of them we will omit this part in order to save time and turn immediately to PyMC, a Python library for performing Bayesian analysis.\n",
    "\n",
    "\n",
    "Introducing our first hammer: PyMC\n",
    "-----\n",
    "\n",
    "PyMC is a Python library for programming Bayesian analysis [3]. It is a fast, well-maintained library. \n",
    "\n",
    "We will model the problem above using PyMC. This type of programming is called *probabilistic programming*.\n",
    "\n",
    "B. Cronin [5] has a very motivating description of probabilistic programming:\n",
    "\n",
    ">   Another way of thinking about this: unlike a traditional program, which only runs in the forward directions, a probabilistic program is run in both the forward and backward direction. It runs forward to compute the consequences of the assumptions it contains about the world (i.e., the model space it represents), but it also runs backward from the data to constrain the possible explanations. In practice, many probabilistic programming systems will cleverly interleave these forward and backward operations to efficiently home in on the best explanations.\n",
    "\n",
    "\n",
    "PyMC code is easy to read. The only novel thing should be the syntax, and I will interrupt the code to explain individual sections. Simply remember that we are representing the model's components ($\\tau, \\lambda_1, \\lambda_2$ ) as variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc065e-cf58-476e-bb7a-4790cce33378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = 1.0/count_data.mean()  # Recall count_data is the\n",
    "                                   # variable that holds our txt counts\n",
    "    lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n",
    "    lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n",
    "    \n",
    "    #tau = pm.DiscreteUniform(\"tau\", lower=0, upper=44)\n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63dc429-6b71-48fa-8860-25732d69b65c",
   "metadata": {},
   "source": [
    "In the code above, we create the PyMC3 variables corresponding to $\\lambda_1$ and $\\lambda_2$. We assign them to PyMC3's *stochastic variables*, so-called because they are treated by the back end as random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed83abb-2441-4913-84a6-45e17652ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    #idx = np.arange(n_count_data) # Index\n",
    "    idx=np.arange(n_count_data)\n",
    "    lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb66e5e-6bee-4913-b747-871d33afe26a",
   "metadata": {},
   "source": [
    "This code creates a new function `lambda_`, but really we can think of it as a random variable: the random variable $\\lambda$ from above. The `switch()` function assigns `lambda_1` or `lambda_2` as the value of `lambda_`, depending on what side of `tau` we are on. The values of `lambda_` up until `tau` are `lambda_1` and the values afterwards are `lambda_2`.\n",
    "\n",
    "Note that because `lambda_1`, `lambda_2` and `tau` are random, `lambda_` will be random. We are **not** fixing any variables yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfd75c-06df-4022-8ce9-d20f8e1e2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    observation = pm.Poisson(\"obs\", lambda_, observed=count_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae94d26-c30d-4ec8-a81f-fd0e3dfce802",
   "metadata": {},
   "source": [
    "The variable `observation` combines our data, `count_data`, with our proposed data-generation scheme, given by the variable `lambda_`, through the `observed` keyword. \n",
    "\n",
    "The machinery employed below for inference is called *Markov Chain Monte Carlo* (MCMC), later we will talk about it in detail. This technique returns thousands of random variables from the posterior distributions of $\\lambda_1, \\lambda_2$ and $\\tau$. We can plot a histogram of the random variables to see what the posterior distributions look like. Below, we collect the samples (called *traces* in the MCMC literature) into histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefdeb1-1260-425c-bb5a-0542c2342322",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(10000, tune=5000, step=step,\n",
    "                      return_inferencedata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627bc118-d452-452c-a8bc-60a441a11a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_1_samples = trace['lambda_1']\n",
    "lambda_2_samples = trace['lambda_2']\n",
    "tau_samples = trace['tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed4f50-9d31-48a7-8feb-7ac94b592e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 10)\n",
    "#histogram of the samples:\n",
    "\n",
    "ax = plt.subplot(311)\n",
    "ax.set_autoscaley_on(False)\n",
    "\n",
    "plt.hist(lambda_1_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_1$\", color=\"#A60628\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(r\"\"\"Posterior distributions of the variables\n",
    "    $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_1$ value\")\n",
    "\n",
    "ax = plt.subplot(312)\n",
    "ax.set_autoscaley_on(False)\n",
    "plt.hist(lambda_2_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_2$\", color=\"#7A68A6\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_2$ value\")\n",
    "\n",
    "plt.subplot(313)\n",
    "w = 1.0 / tau_samples.shape[0] * np.ones_like(tau_samples)\n",
    "plt.hist(tau_samples, bins=n_count_data, alpha=1,\n",
    "         label=r\"posterior of $\\tau$\",\n",
    "         color=\"#467821\", weights=w, rwidth=2.)\n",
    "plt.xticks(np.arange(n_count_data))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, .75])\n",
    "plt.xlim([35, len(count_data)-20])\n",
    "plt.xlabel(r\"$\\tau$ (in days)\")\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d7ff0-b280-459c-94ee-fb5d0ffdbddf",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Recall that Bayesian methodology returns a *distribution*. Hence we now have distributions to describe the unknown $\\lambda$s and $\\tau$. What have we gained? Immediately, we can see the uncertainty in our estimates: the wider the distribution, the less certain our posterior belief should be. We can also see what the plausible values for the parameters are: $\\lambda_1$ is around 18 and $\\lambda_2$ is around 23. The posterior distributions of the two $\\lambda$s are clearly distinct, indicating that it is indeed likely that there was a change in the user's text-message behaviour.\n",
    "\n",
    "What other observations can you make? If you look at the original data again, do these results seem reasonable? \n",
    "\n",
    "Notice also that the posterior distributions for the $\\lambda$s do not look like exponential distributions, even though our priors for these variables were exponential. In fact, the posterior distributions are not really of any form that we recognize from the original model. But that's OK! This is one of the benefits of taking a computational point of view. If we had instead done this analysis using mathematical approaches, we would have been stuck with an analytically intractable (and messy) distribution. Our use of a computational approach makes us indifferent to mathematical tractability.\n",
    "\n",
    "Our analysis also returned a distribution for $\\tau$. Its posterior distribution looks a little different from the other two because it is a discrete random variable, so it doesn't assign probabilities to intervals. We can see that near day 45, there was a 50% chance that the user's behaviour changed. Had no change occurred, or had the change been gradual over time, the posterior distribution of $\\tau$ would have been more spread out, reflecting that many days were plausible candidates for $\\tau$. By contrast, in the actual results we see that only three or four days make any sense as potential transition points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de448e13-a481-4f09-aa0c-4cb7490aea3e",
   "metadata": {},
   "source": [
    "### Why would I want samples from the posterior, anyways?\n",
    "\n",
    "For now we'll use the posterior samples to answer the following question: what is the expected number of texts at day $t, \\; 0 \\le t \\le 70$ ? Recall that the expected value of a Poisson variable is equal to its parameter $\\lambda$. Therefore, the question is equivalent to *what is the expected value of $\\lambda$ at time $t$*?\n",
    "\n",
    "In the code below, let $i$ index samples from the posterior distributions. Given a day $t$, we average over all possible $\\lambda_i$ for that day $t$, using $\\lambda_i = \\lambda_{1,i}$ if $t \\lt \\tau_i$ (that is, if the behaviour change has not yet occurred), else we use $\\lambda_i = \\lambda_{2,i}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68210a5e-4286-46d4-bf19-3c6fe23099ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 5)\n",
    "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "# N samples from the corresponding posterior distribution\n",
    "N = tau_samples.shape[0]\n",
    "expected_texts_per_day = np.zeros(n_count_data)\n",
    "for day in range(0, n_count_data):\n",
    "    # ix is a bool index of all tau samples corresponding to\n",
    "    # the switchpoint occurring prior to value of 'day'\n",
    "    ix = day < tau_samples\n",
    "    # Each posterior sample corresponds to a value for tau.\n",
    "    # for each day, that value of tau indicates whether we're \"before\"\n",
    "    # (in the lambda1 \"regime\") or\n",
    "    #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
    "    # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
    "    # over all samples to get an expected value for lambda on that day.\n",
    "    # As explained, the \"message count\" random variable is Poisson distributed,\n",
    "    # and therefore lambda (the poisson parameter) is the expected value of\n",
    "    # \"message count\".\n",
    "    expected_texts_per_day[day] = (lambda_1_samples[ix].sum()\n",
    "                                   + lambda_2_samples[~ix].sum()) / N\n",
    "\n",
    "\n",
    "plt.plot(range(n_count_data), expected_texts_per_day, lw=4, color=\"#E24A33\",\n",
    "         label=\"expected number of text-messages received\")\n",
    "plt.xlim(0, n_count_data)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Expected # text-messages\")\n",
    "plt.title(\"Expected number of text-messages received\")\n",
    "plt.ylim(0, 60)\n",
    "plt.bar(np.arange(len(count_data)), count_data, color=\"#348ABD\", alpha=0.65,\n",
    "        label=\"observed texts per day\")\n",
    "\n",
    "plt.legend(loc=\"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632db51-9bd2-4097-bc56-0494081fe72b",
   "metadata": {},
   "source": [
    "Our analysis shows strong support for believing the user's behavior did change ($\\lambda_1$ would have been close in value to $\\lambda_2$ had this not been true), and that the change was sudden rather than gradual (as demonstrated by $\\tau$'s strongly peaked posterior distribution). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688898c0-f4d5-4045-b5ee-726c2e3f1d62",
   "metadata": {},
   "source": [
    "##### Exercises\n",
    "\n",
    "1\\.  Using `lambda_1_samples` and `lambda_2_samples`, what is the mean of the posterior distributions of $\\lambda_1$ and $\\lambda_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a63e65-75b8-4561-9690-480f6a067523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here.\n",
    "print(lambda_1_samples.mean(),lambda_2_samples.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fcbaf9-7bfa-456f-bcec-845584a8ac80",
   "metadata": {},
   "source": [
    "2\\.  What is the expected percentage increase in text-message rates? `hint:` compute the mean of `lambda_1_samples/lambda_2_samples`. Note that this quantity is very different from `lambda_1_samples.mean()/lambda_2_samples.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03b8aa2-d2b3-444d-aa5e-c3ddc9bc3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here.\n",
    "a=1-lambda_1_samples/lambda_2_samples\n",
    "a.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a4da1-727a-437b-8926-c146c96cc0cf",
   "metadata": {},
   "source": [
    "3\\. What is the mean of $\\lambda_1$ **given** that we know $\\tau$ is less than 45.  That is, suppose we have been given new information that the change in behaviour occurred prior to day 45. What is the expected value of $\\lambda_1$ now? (You do not need to redo the PyMC3 part. Just consider all instances where `tau_samples < 45`.) What if we actually re-run the code with this information? (technically this should be specified in the sampling algorithm, but we will bound ourselves to prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2d1b3-91ef-47da-99d3-c252b0c16432",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 5)\n",
    "# tau_samples, lambda_1_samples, lambda_2_samples contain\n",
    "# N samples from the corresponding posterior distribution\n",
    "lambda_1_samples=lambda_1_samples[tau_samples<45]\n",
    "lambda_2_samples=lambda_2_samples[tau_samples<45]\n",
    "tau_samples=tau_samples[tau_samples<45]\n",
    "\n",
    "#N = tau_samples.shape[0]\n",
    "N=tau_samples.shape[0]\n",
    "expected_texts_per_day = np.zeros(n_count_data)\n",
    "for day in range(0, n_count_data):\n",
    "    # ix is a bool index of all tau samples corresponding to\n",
    "    # the switchpoint occurring prior to value of 'day'\n",
    "    ix = day < tau_samples\n",
    "    # Each posterior sample corresponds to a value for tau.\n",
    "    # for each day, that value of tau indicates whether we're \"before\"\n",
    "    # (in the lambda1 \"regime\") or\n",
    "    #  \"after\" (in the lambda2 \"regime\") the switchpoint.\n",
    "    # by taking the posterior sample of lambda1/2 accordingly, we can average\n",
    "    # over all samples to get an expected value for lambda on that day.\n",
    "    # As explained, the \"message count\" random variable is Poisson distributed,\n",
    "    # and therefore lambda (the poisson parameter) is the expected value of\n",
    "    # \"message count\".\n",
    "    expected_texts_per_day[day] = (lambda_1_samples[ix].sum()\n",
    "                                   + lambda_2_samples[~ix].sum()) / N\n",
    "    \n",
    "#print(len(expected_texts_per_day),\" texts \")\n",
    "\n",
    "\n",
    "plt.plot(range(n_count_data), expected_texts_per_day, lw=4, color=\"#E24A33\",\n",
    "         label=\"expected number of text-messages received\")\n",
    "plt.xlim(0, n_count_data)\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Expected # text-messages\")\n",
    "plt.title(\"Expected number of text-messages received\")\n",
    "plt.ylim(0, 60)\n",
    "plt.bar(np.arange(len(count_data)), count_data, color=\"#348ABD\", alpha=0.65,\n",
    "        label=\"observed texts per day\")\n",
    "\n",
    "plt.legend(loc=\"upper left\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4fc72d-680f-449b-b65b-d94fcf1aa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_1_samples.mean(),lambda_2_samples.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c7287-aef2-4d36-8f60-cd98a2e650f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###with re-running\n",
    "\n",
    "\n",
    "\n",
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha = 1.0/count_data.mean()  # Recall count_data is the\n",
    "                                   # variable that holds our txt counts\n",
    "    lambda_1 = pm.Exponential(\"lambda_1\", alpha)\n",
    "    lambda_2 = pm.Exponential(\"lambda_2\", alpha)\n",
    "    \n",
    "    tau = pm.DiscreteUniform(\"tau\", lower=0, upper=44)\n",
    "    #tau = pm.DiscreteUniform(\"tau\", lower=0, upper=n_count_data-1)\n",
    "    \n",
    "#with model:\n",
    "    #idx = np.arange(n_count_data) # Index\n",
    "    idx=np.arange(n_count_data)\n",
    "    lambda_ = pm.math.switch(tau > idx, lambda_1, lambda_2)\n",
    "    \n",
    "#with model:\n",
    "    observation = pm.Poisson(\"obs\", lambda_, observed=count_data)\n",
    "\n",
    "#with model:\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(10000, tune=5000, step=step,\n",
    "                      return_inferencedata=False)\n",
    "    \n",
    "lambda_1_samples = trace['lambda_1']\n",
    "lambda_2_samples = trace['lambda_2']\n",
    "tau_samples = trace['tau']\n",
    "\n",
    "print(lambda_1_samples.mean(),lambda_2_samples.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2172796-70a6-4240-858b-8fb36a0b7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(12.5, 10)\n",
    "#histogram of the samples:\n",
    "\n",
    "ax = plt.subplot(311)\n",
    "ax.set_autoscaley_on(False)\n",
    "\n",
    "plt.hist(lambda_1_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_1$\", color=\"#A60628\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(r\"\"\"Posterior distributions of the variables\n",
    "    $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_1$ value\")\n",
    "\n",
    "ax = plt.subplot(312)\n",
    "ax.set_autoscaley_on(False)\n",
    "plt.hist(lambda_2_samples, histtype='stepfilled', bins=30, alpha=0.85,\n",
    "         label=\"posterior of $\\lambda_2$\", color=\"#7A68A6\", density=True)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim([15, 30])\n",
    "plt.xlabel(\"$\\lambda_2$ value\")\n",
    "\n",
    "plt.subplot(313)\n",
    "w = 1.0 / tau_samples.shape[0] * np.ones_like(tau_samples)\n",
    "plt.hist(tau_samples, bins=n_count_data, alpha=1,\n",
    "         label=r\"posterior of $\\tau$\",\n",
    "         color=\"#467821\", weights=w, rwidth=2.)\n",
    "plt.xticks(np.arange(n_count_data))\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, .75])\n",
    "plt.xlim([35, len(count_data)-20])\n",
    "plt.xlabel(r\"$\\tau$ (in days)\")\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783d7c7-8f13-41d1-b00e-d077b4c7e718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-2023.5",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
